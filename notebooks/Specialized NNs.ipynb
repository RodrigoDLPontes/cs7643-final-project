{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "This notebook was created to be run in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1oX00k67BHEt",
    "outputId": "81faa95c-ea42-4c71-c421-8aff1201bda5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# Get GRAM data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "!cp '/content/gdrive/My Drive/GRAM.tar.xz' .\n",
    "!tar --extract --xz -f GRAM.tar.xz\n",
    "drive.flush_and_unmount()\n",
    "!rm GRAM.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t8TN35bSBQ7C",
    "outputId": "dea704a7-ba5f-43cd-ca3d-b88a1fa63cae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cs7643-final-project'...\n",
      "remote: Enumerating objects: 38, done.\u001b[K\n",
      "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
      "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
      "remote: Total 38 (delta 12), reused 36 (delta 10), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (38/38), done.\n"
     ]
    }
   ],
   "source": [
    "# Clone code repo\n",
    "!git clone https://github.com/RodrigoDLPontes/cs7643-final-project.git\n",
    "!mv cs7643-final-project/* .\n",
    "!rm -rf cs7643-final-project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFEcWZaxBTSy"
   },
   "outputs": [],
   "source": [
    "# Create GRAM PyTorch Dataset object\n",
    "from gram import GRAM_RTM\n",
    "gram_dataset = GRAM_RTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJkWUzvdB4fW"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZuzPZxiB4FJ"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 15 # Max number of cars found in a single picture in training data\n",
    "NUM_FEATURES = 64 # Don't change cuz this the default value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XrwSNoKyBVNW"
   },
   "outputs": [],
   "source": [
    "class MyModuleList(nn.ModuleList):\n",
    "    def __add__(self, x):\n",
    "        tmp = [m for m in self.modules()] + [m for m in x.modules()]\n",
    "        return MyModuleList(tmp)\n",
    "    def forward(self, x):\n",
    "        for layer in self:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "def make_basic_block(inplanes, planes, stride=1, downsample=None):\n",
    "    def conv3x3(in_planes, out_planes, stride=1):\n",
    "        return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                         padding=1, bias=False)\n",
    "\n",
    "    block_list = MyModuleList([\n",
    "            conv3x3(inplanes, planes, stride),\n",
    "            nn.BatchNorm2d(planes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv3x3(planes, planes),\n",
    "            nn.BatchNorm2d(planes),\n",
    "    ])\n",
    "    if downsample == None:\n",
    "        residual = MyModuleList([])\n",
    "    else:\n",
    "        residual = downsample\n",
    "    return MyModuleList([block_list, residual])\n",
    "\n",
    "#Specialized NN\n",
    "class PytorchResNet(nn.Module):\n",
    "    def __init__(self, section_reps,\n",
    "                 num_classes=NUM_CLASSES, nbf=NUM_FEATURES,\n",
    "                 conv1_size=7, conv1_pad=3,\n",
    "                 downsample_start=True):\n",
    "        super(PytorchResNet, self).__init__()\n",
    "\n",
    "        # Since use_basic_block == True\n",
    "        self.expansion = 1\n",
    "        self.block_fn = make_basic_block\n",
    "\n",
    "        self.downsample_start = downsample_start\n",
    "        self.inplanes = nbf\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, nbf, kernel_size=conv1_size,\n",
    "                               stride=downsample_start + 1, padding=conv1_pad, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(nbf)\n",
    "\n",
    "        sections = []\n",
    "        for i, section_rep in enumerate(section_reps):\n",
    "            sec = self._make_section(nbf * (2 ** i), section_rep, stride=(i != 0) + 1)\n",
    "            sections.append(sec)\n",
    "        self.sections = MyModuleList(sections)\n",
    "        lin_inp = nbf * int(2 ** (len(section_reps) - 1)) * self.expansion \\\n",
    "            if len(self.sections) != 0 else nbf\n",
    "        self.fc = nn.Linear(lin_inp, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_section(self, planes, num_blocks, stride=1):\n",
    "        if stride != 1 or self.inplanes != planes * self.expansion:\n",
    "            downsample = MyModuleList([\n",
    "                    nn.Conv2d(self.inplanes, planes * self.expansion,\n",
    "                              kernel_size=1, stride=stride, bias=False),\n",
    "                    nn.BatchNorm2d(planes * self.expansion),\n",
    "            ])\n",
    "        else:\n",
    "            downsample = None\n",
    "\n",
    "        blocks = []\n",
    "        blocks.append(self.block_fn(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * self.expansion\n",
    "        for i in range(1, num_blocks):\n",
    "            blocks.append(self.block_fn(self.inplanes, planes))\n",
    "\n",
    "        return MyModuleList(blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        if self.downsample_start:\n",
    "            x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        for sec_ind, section in enumerate(self.sections):\n",
    "            for block_ind, (block, shortcut) in enumerate(section):\n",
    "                x_input = x\n",
    "                if len(shortcut) != 0:\n",
    "                    x = shortcut(x)\n",
    "                x_conv = block(x_input)\n",
    "                x = x + x_conv\n",
    "                x = F.relu(x)\n",
    "\n",
    "        x = F.avg_pool2d(x, (x.size(2), x.size(3)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rho9aJtUBwoz"
   },
   "outputs": [],
   "source": [
    "base_model = PytorchResNet([1,1,1,1], num_classes=7, conv1_size=3, conv1_pad=1, nbf=16, downsample_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NcQGE3JsCGMB",
    "outputId": "d930a627-ac3e-4bd4-da39-94f62137e532"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PytorchResNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (sections): MyModuleList(\n",
      "    (0): MyModuleList(\n",
      "      (0): MyModuleList(\n",
      "        (0): MyModuleList(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): MyModuleList()\n",
      "      )\n",
      "    )\n",
      "    (1): MyModuleList(\n",
      "      (0): MyModuleList(\n",
      "        (0): MyModuleList(\n",
      "          (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): MyModuleList(\n",
      "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): MyModuleList(\n",
      "      (0): MyModuleList(\n",
      "        (0): MyModuleList(\n",
      "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): MyModuleList(\n",
      "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): MyModuleList(\n",
      "      (0): MyModuleList(\n",
      "        (0): MyModuleList(\n",
      "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): MyModuleList(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0TZAxDjD50X"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "gram_dataset = GRAM_RTM(spec_nn=True)\n",
    "# Random split of data into training and validation\n",
    "# Datasets\n",
    "train_dataset = GRAM_RTM(spec_nn=True, split='train')\n",
    "val_dataset = GRAM_RTM(spec_nn=True, split='val')\n",
    "test_dataset = GRAM_RTM(spec_nn=True, split='test')\n",
    "# DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                 batch_size=16, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                 batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                 batch_size=16, shuffle=True)\n",
    "# loader = DataLoader(gram_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLJiDfGfD64E"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "rcnn = base_model.cuda()\n",
    "sgd = optim.SGD(rcnn.parameters(), lr=0.1, momentum=0.9)\n",
    "criterion = F.cross_entropy\n",
    "\n",
    "def train(epoch, losses):\n",
    "    '''\n",
    "    Train the model for one epoch.\n",
    "    '''\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        images, targets = batch['image'].cuda(), batch['num_cars'].cuda()\n",
    "        sgd.zero_grad()\n",
    "        output = rcnn(images)\n",
    "        loss = criterion(output, targets)\n",
    "        np_output = output[0].detach().cpu().numpy()\n",
    "        print(f'\\r{epoch}: {loss.item():.2f}', end='')\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        sgd.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KeUlQVi_6jLa"
   },
   "outputs": [],
   "source": [
    "def evaluate(epoch, split):\n",
    "    '''\n",
    "    Compute loss on val or test data.\n",
    "    '''\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    n_examples = 0\n",
    "    with torch.no_grad():\n",
    "        if split == 'val':\n",
    "            loader = val_loader\n",
    "        elif split == 'test':\n",
    "            loader = test_loader\n",
    "        for batch_idx, batch in enumerate(loader):\n",
    "            act_maps, targets = batch['image'].cuda(), batch['num_cars'].cuda()\n",
    "            output = rcnn(act_maps)\n",
    "            loss += criterion(output, targets).item()\n",
    "            pred = torch.argmax(output, dim=1)\n",
    "            if split and batch_idx == 0 == 'test': print(pred)\n",
    "            correct += (pred == targets).cpu().sum().item()\n",
    "            n_examples += pred.shape[0]\n",
    "        loss /= n_examples\n",
    "        acc = 100. * correct / n_examples\n",
    "        print(f'{epoch}: {split} set: Average loss: {loss:.4f}, Accuracy: {correct}/{n_examples} ({acc:.0f}%)')\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "id": "-Tp8gFfZD-6X",
    "outputId": "6f572489-34c9-44b0-873b-0e6327793c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 1.061: val set: Average loss: 0.1020, Accuracy: 698/1504 (46%)\n",
      "2: 0.872: val set: Average loss: 0.1010, Accuracy: 529/1504 (35%)\n",
      "3: 0.863: val set: Average loss: 0.1028, Accuracy: 692/1504 (46%)\n",
      "4: 0.924: val set: Average loss: 0.1038, Accuracy: 723/1504 (48%)\n",
      "5: 0.555: val set: Average loss: 0.1066, Accuracy: 649/1504 (43%)\n",
      "6: 0.496: val set: Average loss: 0.1028, Accuracy: 784/1504 (52%)\n",
      "7: 0.207: val set: Average loss: 0.1138, Accuracy: 839/1504 (56%)\n",
      "8: 0.638: val set: Average loss: 0.1061, Accuracy: 835/1504 (56%)\n",
      "9: 0.719: val set: Average loss: 0.1241, Accuracy: 677/1504 (45%)\n",
      "10: 0.4510: val set: Average loss: 0.1394, Accuracy: 783/1504 (52%)\n",
      "11: 0.3311: val set: Average loss: 0.1236, Accuracy: 919/1504 (61%)\n",
      "12: 0.4112: val set: Average loss: 0.1682, Accuracy: 717/1504 (48%)\n",
      "13: 0.0313: val set: Average loss: 0.1568, Accuracy: 813/1504 (54%)\n",
      "14: 0.0214: val set: Average loss: 0.1274, Accuracy: 847/1504 (56%)\n",
      "15: 0.0715: val set: Average loss: 0.1515, Accuracy: 714/1504 (47%)\n",
      "16: 0.0416: val set: Average loss: 0.1552, Accuracy: 906/1504 (60%)\n",
      "17: 0.0517: val set: Average loss: 0.1453, Accuracy: 884/1504 (59%)\n",
      "18: 0.0818: val set: Average loss: 0.1326, Accuracy: 937/1504 (62%)\n",
      "19: 0.0219: val set: Average loss: 0.1682, Accuracy: 882/1504 (59%)\n",
      "20: 0.0520: val set: Average loss: 0.1475, Accuracy: 868/1504 (58%)\n",
      "21: 0.0221: val set: Average loss: 0.1713, Accuracy: 901/1504 (60%)\n",
      "22: 0.0122: val set: Average loss: 0.1640, Accuracy: 823/1504 (55%)\n",
      "23: 0.0123: val set: Average loss: 0.1615, Accuracy: 912/1504 (61%)\n",
      "24: 0.0124: val set: Average loss: 0.1434, Accuracy: 868/1504 (58%)\n",
      "25: 0.0125: val set: Average loss: 0.1675, Accuracy: 879/1504 (58%)\n",
      "26: 0.3326: val set: Average loss: 0.1692, Accuracy: 926/1504 (62%)\n",
      "27: 0.0227: val set: Average loss: 0.1569, Accuracy: 919/1504 (61%)\n",
      "28: 0.0128: val set: Average loss: 0.1781, Accuracy: 929/1504 (62%)\n",
      "29: 0.0029: val set: Average loss: 0.1748, Accuracy: 911/1504 (61%)\n",
      "30: 0.0030: val set: Average loss: 0.1788, Accuracy: 943/1504 (63%)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-156e3de30513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: evaluate() missing 1 required positional argument: 'split'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "train_losses = []\n",
    "NUM_EPOCHS = 30\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train(epoch, train_losses)\n",
    "    evaluate(epoch, 'val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5zOeCSWEIfg",
    "outputId": "5655a491-66ca-4951-e9d4-962c350e78f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30: test set: Average loss: 0.2690, Accuracy: 724/1504 (48%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2689679332394549, 48.138297872340424)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(epoch=30, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "-W5L5ozrEBqn",
    "outputId": "0486021b-df4e-4bbb-c218-c30e8c8eb3e8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f8ef7fb82117>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_losses)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "7643_Specialized_NN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
